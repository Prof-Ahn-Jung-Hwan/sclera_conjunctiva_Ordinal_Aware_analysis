#!/usr/bin/env python3
"""
Image-bins Model Selection from Hyperparameter Optimization Results
================================================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” image-bins í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
Bayesian optimizationìœ¼ë¡œ ìƒì„±ëœ ë‹¤ì–‘í•œ bins ê°’ë“¤ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.

ì‘ì„±ì¼: 2025-09-22
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
import os
import glob
from pathlib import Path
import matplotlib.pyplot as plt

def load_and_analyze_image_bins_models():
    """combineResults_from_ImageBinsHPO í´ë”ì˜ ëª¨ë“  image-bins ëª¨ë¸ì„ ë¶„ì„"""
    print("ğŸš€ Image-bins Model Selection from Bayesian Optimization Results")
    print("=" * 70)

    # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
    os.chdir('/home/erdrajh/project_a6000/project25/2506Anemia/kimsangwon_code_3')

    # combineResults_from_ImageBinsHPO í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì°¾ê¸°
    results_pattern = 'combineResults_from_ImageBinsHPO/results_image-bins*_combined.xlsx'
    files = sorted(glob.glob(results_pattern))

    if not files:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_pattern}")
        return None

    print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ë“¤: {len(files)}ê°œ")
    for i, file in enumerate(files, 1):
        bins_num = extract_bins_from_filename(file)
        print(f"   {i:2d}. image-bins{bins_num} ({os.path.basename(file)})")

    # ëª¨ë“  íŒŒì¼ ë¡œë“œ ë° ë¶„ì„
    all_results = []
    
    for file in files:
        print(f"\nğŸ“– ë¶„ì„ ì¤‘: {os.path.basename(file)}")
        result = analyze_single_file(file)
        if result is not None:
            all_results.append(result)

    if not all_results:
        print("âŒ ë¶„ì„í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(all_results)
    
    return df

def extract_bins_from_filename(filename):
    """íŒŒì¼ëª…ì—ì„œ bins ê°’ ì¶”ì¶œ"""
    import re
    match = re.search(r'image-bins(\d+)', filename)
    return int(match.group(1)) if match else 0

def analyze_single_file(file_path):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
    try:
        df = pd.read_excel(file_path)
        
        if 'ground truth' not in df.columns or 'prediction' not in df.columns:
            print(f"   âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return None

        # Bins ê°’ ì¶”ì¶œ
        bins = extract_bins_from_filename(file_path)
        
        # ê° foldë³„ MAE ê³„ì‚°
        fold_maes = {}
        fold_counts = {}

        for fold in range(5):  # 0-4 folds
            fold_data = df[df['fold'] == fold]
            if len(fold_data) > 0:
                mae = np.mean(np.abs(fold_data['ground truth'] - fold_data['prediction']))
                fold_maes[fold] = mae
                fold_counts[fold] = len(fold_data)

        if not fold_maes:
            print(f"   âŒ fold ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return None

        # í†µê³„ ê³„ì‚°
        fold_mae_values = list(fold_maes.values())
        mean_mae = np.mean(fold_mae_values)
        std_mae = np.std(fold_mae_values, ddof=1)
        
        # RMSE ê³„ì‚°
        rmse = np.sqrt(np.mean((df['ground truth'] - df['prediction'])**2))
        
        # CV (ë³€ë™ê³„ìˆ˜) ê³„ì‚°
        cv = std_mae / mean_mae if mean_mae > 0 else 0
        
        # 95% CI ê³„ì‚°
        if len(fold_mae_values) > 1:
            ci_range = stats.t.interval(0.95, len(fold_mae_values)-1, 
                                      loc=mean_mae, scale=stats.sem(fold_mae_values))
            ci_width = ci_range[1] - ci_range[0]
            ci_range_str = f"[{ci_range[0]:.4f}, {ci_range[1]:.4f}]"
        else:
            ci_width = 0
            ci_range_str = "[0.0000, 0.0000]"

        # Composite Score ê³„ì‚° (ì„±ëŠ¥ + ì•ˆì •ì„± + ë‹¨ìˆœì„±)
        composite_score = mean_mae + 0.5 * std_mae + 0.1 * (bins / 100)

        result = {
            'Experiment': f'image-bins{bins}',
            'Bins': bins,
            'Mean_MAE': mean_mae,
            'Std_Dev': std_mae,
            'RMSE': rmse,
            'CV': cv,
            '95%_CI_Range': ci_range_str,
            'CI_Width': ci_width,
            'Composite_Score': composite_score,
            'Total_Samples': len(df),
            'Fold_Counts': fold_counts,
            'Fold_MAEs': fold_maes,
            'Source_File': file_path
        }

        print(f"   âœ… bins{bins}: MAE {mean_mae:.4f} Â± {std_mae:.4f}")
        
        return result

    except Exception as e:
        print(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def analyze_best_models(df):
    """ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ë¶„ì„"""
    print('\nğŸ“Š ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¡œ Best Model ë¶„ì„')
    print('=' * 60)

    # 1. MAE ê¸°ì¤€ (Bayesian Optimizationì˜ ëª©í‘œ)
    print('\n1. ğŸ¯ MAE ê¸°ì¤€ (Bayesian Optimization ëª©í‘œ):')
    mae_best = df.loc[df['Mean_MAE'].idxmin()]
    print(f'   ğŸ† {mae_best["Experiment"]}')
    print(f'   ğŸ“ˆ MAE: {mae_best["Mean_MAE"]:.4f} Â± {mae_best["Std_Dev"]:.4f}')
    print(f'   ğŸ”¢ Bins: {mae_best["Bins"]}')

    # 2. RMSE ê¸°ì¤€
    print('\n2. ğŸ“ RMSE ê¸°ì¤€ (í° ì˜¤ì°¨ì— ë” í° íŒ¨ë„í‹°):')
    rmse_best = df.loc[df['RMSE'].idxmin()]
    print(f'   ğŸ† {rmse_best["Experiment"]}')
    print(f'   ğŸ“ˆ RMSE: {rmse_best["RMSE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {rmse_best["Bins"]}')

    # 3. Stability ê¸°ì¤€
    print('\n3. ğŸ­ Stability ê¸°ì¤€ (í‘œì¤€í¸ì°¨ê°€ ê°€ì¥ ì‘ì€ ëª¨ë¸):')
    stability_best = df.loc[df['Std_Dev'].idxmin()]
    print(f'   ğŸ† {stability_best["Experiment"]}')
    print(f'   ğŸ“Š Std: {stability_best["Std_Dev"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {stability_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {stability_best["Bins"]}')

    # 4. CV ê¸°ì¤€
    print('\n4. ğŸ“Š CV (ë³€ë™ê³„ìˆ˜) ê¸°ì¤€:')
    cv_best = df.loc[df['CV'].idxmin()]
    print(f'   ğŸ† {cv_best["Experiment"]}')
    print(f'   ğŸ“ˆ CV: {cv_best["CV"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {cv_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {cv_best["Bins"]}')

    # 5. Composite Score
    print('\n5. ğŸ… Composite Score (ì„±ëŠ¥ + ì•ˆì •ì„± + ë‹¨ìˆœì„±):')
    composite_best = df.loc[df['Composite_Score'].idxmin()]
    print(f'   ğŸ† {composite_best["Experiment"]}')
    print(f'   ğŸ“Š Score: {composite_best["Composite_Score"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {composite_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {composite_best["Bins"]}')

    # 6. CI Width ê¸°ì¤€
    print('\n6. ğŸ¯ 95% CI ë²”ìœ„ê°€ ê°€ì¥ ì¢ì€ ëª¨ë¸ (ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”):')
    ci_best = df.loc[df['CI_Width'].idxmin()]
    print(f'   ğŸ† {ci_best["Experiment"]}')
    print(f'   ğŸ“Š CI Width: {ci_best["CI_Width"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {ci_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {ci_best["Bins"]}')

    # 7. íš¨ìœ¨ì„± ê¸°ì¤€ (ì¢‹ì€ ì„±ëŠ¥ + ìµœì†Œ bins)
    print('\n7. âš¡ íš¨ìœ¨ì„± ê¸°ì¤€ (ì¢‹ì€ ì„±ëŠ¥ + ìµœì†Œ bins):')
    # MAEê°€ ìƒìœ„ 25% ì•ˆì— ë“¤ë©´ì„œ binsê°€ ê°€ì¥ ì‘ì€ ëª¨ë¸
    mae_threshold = df['Mean_MAE'].quantile(0.25)
    efficient_models = df[df['Mean_MAE'] <= mae_threshold]
    if not efficient_models.empty:
        efficient_best = efficient_models.loc[efficient_models['Bins'].idxmin()]
        print(f'   ğŸ† {efficient_best["Experiment"]}')
        print(f'   ğŸ“ˆ MAE: {efficient_best["Mean_MAE"]:.4f} (ìƒìœ„ 25%)')
        print(f'   ğŸ”¢ Bins: {efficient_best["Bins"]} (ìµœì†Œ)')
    else:
        efficient_best = df.loc[df['Bins'].idxmin()]
        print(f'   ğŸ† {efficient_best["Experiment"]} (ìµœì†Œ bins)')
        print(f'   ğŸ“ˆ MAE: {efficient_best["Mean_MAE"]:.4f}')
        print(f'   ğŸ”¢ Bins: {efficient_best["Bins"]}')

    return {
        'mae_best': mae_best,
        'rmse_best': rmse_best,
        'stability_best': stability_best,
        'cv_best': cv_best,
        'composite_best': composite_best,
        'ci_best': ci_best,
        'efficient_best': efficient_best
    }

def show_top_models_summary(df, top_n=5):
    """ìƒìœ„ ëª¨ë¸ë“¤ ìš”ì•½ í‘œì‹œ"""
    print(f'\nğŸ“‹ Top {top_n} Models Summary')
    print('=' * 80)
    
    # MAE ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    top_models = df.nsmallest(top_n, 'Mean_MAE')
    
    print(f"{'Rank':<4} {'Model':<12} {'MAE':<8} {'Std':<8} {'RMSE':<8} {'CV':<8} {'Bins':<6} {'Composite':<10}")
    print('-' * 80)
    
    for i, (_, model) in enumerate(top_models.iterrows(), 1):
        print(f"{i:<4} {model['Experiment']:<12} {model['Mean_MAE']:<8.4f} "
              f"{model['Std_Dev']:<8.4f} {model['RMSE']:<8.4f} {model['CV']:<8.4f} "
              f"{model['Bins']:<6} {model['Composite_Score']:<10.4f}")

def analyze_bins_vs_performance_trend(df):
    """Bins ê°’ê³¼ ì„±ëŠ¥ì˜ ê´€ê³„ ë¶„ì„"""
    print(f'\nğŸ“ˆ Bins vs Performance ê´€ê³„ ë¶„ì„')
    print('=' * 60)
    
    # Binsë¡œ ì •ë ¬
    df_sorted = df.sort_values('Bins')
    
    # ìƒê´€ê´€ê³„ ë¶„ì„
    bins_mae_corr = df['Bins'].corr(df['Mean_MAE'])
    bins_std_corr = df['Bins'].corr(df['Std_Dev'])
    
    print(f'ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„:')
    print(f'   â€¢ Bins vs MAE ìƒê´€ê³„ìˆ˜: {bins_mae_corr:.3f}')
    print(f'   â€¢ Bins vs Std ìƒê´€ê³„ìˆ˜: {bins_std_corr:.3f}')
    
    print(f'\nğŸ“Š Bins êµ¬ê°„ë³„ ì„±ëŠ¥:')
    
    # Binsë¥¼ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
    bins_ranges = [
        (0, 20, "Very Low (0-20)"),
        (21, 40, "Low (21-40)"), 
        (41, 60, "Medium (41-60)"),
        (61, 80, "High (61-80)"),
        (81, 100, "Very High (81-100)")
    ]
    
    for min_bins, max_bins, label in bins_ranges:
        range_data = df[(df['Bins'] >= min_bins) & (df['Bins'] <= max_bins)]
        if not range_data.empty:
            avg_mae = range_data['Mean_MAE'].mean()
            avg_std = range_data['Std_Dev'].mean()
            best_in_range = range_data.loc[range_data['Mean_MAE'].idxmin()]
            print(f'   â€¢ {label}: Avg MAE {avg_mae:.4f}, Best: {best_in_range["Experiment"]} ({best_in_range["Mean_MAE"]:.4f})')

def final_recommendation(df, best_models):
    """ìµœì¢… ì¶”ì²œ"""
    print(f'\nğŸ¯ ìµœì¢… ì¶”ì²œ ë° ê²°ë¡ ')
    print('=' * 60)
    
    # Bayesian Optimizationì˜ ëª©í‘œì¸ MAE ê¸°ì¤€ ìµœê³  ëª¨ë¸
    mae_best = best_models['mae_best']
    
    print(f'ğŸ† Bayesian Optimization ëª©í‘œ ë‹¬ì„±:')
    print(f'   âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {mae_best["Experiment"]}')
    print(f'   ğŸ“ˆ MAE: {mae_best["Mean_MAE"]:.4f} Â± {mae_best["Std_Dev"]:.4f}')
    print(f'   ğŸ”¢ Bins: {mae_best["Bins"]}')
    print(f'   ğŸ“Š ì „ì²´ {len(df)}ê°œ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥')
    
    # ì„±ëŠ¥ vs íš¨ìœ¨ì„± trade-off ë¶„ì„
    composite_best = best_models['composite_best']
    
    print(f'\nğŸ­ ê· í˜• ê³ ë ¤ ì¶”ì²œ:')
    if mae_best['Experiment'] == composite_best['Experiment']:
        print(f'   âœ… ì„±ëŠ¥ê³¼ ê· í˜•ì´ ëª¨ë‘ ìµœê³ ì¸ ëª¨ë¸: {mae_best["Experiment"]}')
        print(f'   ğŸ’¡ ë‹¨ì¼ ëª¨ë¸ë¡œ ìµœì  ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ë™ì‹œ ë‹¬ì„±')
    else:
        print(f'   âš–ï¸ ê· í˜• ìµœê³  ëª¨ë¸: {composite_best["Experiment"]}')
        print(f'   ğŸ“ˆ MAE: {composite_best["Mean_MAE"]:.4f} (vs ìµœê³  {mae_best["Mean_MAE"]:.4f})')
        print(f'   ğŸ”¢ Bins: {composite_best["Bins"]} (vs ìµœê³  {mae_best["Bins"]})')
        
        mae_diff = composite_best["Mean_MAE"] - mae_best["Mean_MAE"]
        bins_diff = mae_best["Bins"] - composite_best["Bins"]
        
        print(f'   ğŸ“Š Trade-off: MAE {mae_diff:+.4f} g/dL ì†ì‹¤ë¡œ {bins_diff} bins ì ˆì•½')
    
    # ìµœì¢… ì¶”ì²œ
    print(f'\nğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ:')
    print(f'   ğŸ¯ ìµœê³  ì„±ëŠ¥ í•„ìš”ì‹œ: {mae_best["Experiment"]} (MAE {mae_best["Mean_MAE"]:.4f})')
    print(f'   âš¡ íš¨ìœ¨ì„± ê³ ë ¤ì‹œ: {best_models["efficient_best"]["Experiment"]} (Bins {best_models["efficient_best"]["Bins"]})')
    print(f'   ğŸ­ ì•ˆì •ì„± ìš°ì„ ì‹œ: {best_models["stability_best"]["Experiment"]} (Std {best_models["stability_best"]["Std_Dev"]:.4f})')
    print(f'   ğŸ… ì¢…í•© ê· í˜•: {composite_best["Experiment"]} (Score {composite_best["Composite_Score"]:.4f})')

def improved_model_selection_for_image_bins(df):
    """
    ê°œì„ ëœ ê¸°ì¤€(ë³µì¡ë„, ì‹œë‚˜ë¦¬ì˜¤)ì„ ì ìš©í•˜ì—¬ image-bins ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
    """
    # 1. ê¸°ë³¸ ì§€í‘œ ì¶”ê°€ ê³„ì‚°
    if 'CV' not in df.columns:
        df['CV'] = df['Std_Dev'] / df['Mean_MAE']
    
    # CI_Width ê³„ì‚° (95% CI ë²”ìœ„ì—ì„œ ì¶”ì¶œ)
    df['CI_Width'] = df['95%_CI_Range'].str.extract(r'\[([0-9.]+),\s*([0-9.]+)\]').astype(float).apply(
        lambda x: x[1] - x[0], axis=1
    )
    
    # 2. ğŸ”¥ Image-bins ì „ìš© Complexity ê³„ì‚°
    # image-bins ëª¨ë¸ì€ ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹°ì´ë¯€ë¡œ ë³µì¡ë„ëŠ” ì£¼ë¡œ bins ìˆ˜ì— ì˜ì¡´
    df['Complexity'] = df['Bins'] / df['Bins'].max()
    
    # 3. ìˆœìœ„ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    rank_columns = ['Mean_MAE', 'Std_Dev', 'CV', 'CI_Width', 'Complexity']
    for col in rank_columns:
        df[f'{col}_Rank'] = df[col].rank(method='min')

    num_models = len(df)
    recommendations = {exp: {} for exp in df['Experiment']}

    # 4. ğŸ”¥ Image-bins ì „ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜
    scenarios = {
        'performance_priority': {  # ì—°êµ¬/ë²¤ì¹˜ë§ˆí‚¹: ì„±ëŠ¥ì´ ê°€ì¥ ì¤‘ìš”
            'Mean_MAE': 0.8, 'Std_Dev': 0.1, 'Complexity': 0.1
        },
        'clinical_screening': {    # ğŸ”¥ ì„ìƒ ì„ ë³„ê²€ì‚¬: ì‹¤ìš©ì„±(ë‹¨ìˆœì„±)ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì´ ì¤‘ìš”
            'Mean_MAE': 0.4, 'CV': 0.2, 'Complexity': 0.4
        },
        'stability_priority': {   # ì•ˆì •ì„± ìš°ì„ : ì˜ˆì¸¡ì˜ ì¼ê´€ì„±ê³¼ ì‹ ë¢°ì„±ì´ ê°€ì¥ ì¤‘ìš”
            'Mean_MAE': 0.3, 'CV': 0.3, 'CI_Width': 0.4
        }
    }

    # 5. ì ìˆ˜ ê³„ì‚°
    for scenario_name, weights in scenarios.items():
        for _, model in df.iterrows():
            score = 0
            for metric, weight in weights.items():
                rank_col = f'{metric}_Rank'
                # ìˆœìœ„ë¥¼ 0-1 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ ë³€í™˜ (1ìœ„ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜)
                normalized_score = (num_models - model[rank_col] + 1) / num_models
                score += normalized_score * weight

            recommendations[model['Experiment']][scenario_name] = score

    return recommendations, df

def display_new_criteria_results(df, recommendations):
    """ê°œì„ ëœ ê¸°ì¤€ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì •ë¦¬í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤."""

    scenarios_desc = {
        'performance_priority': 'ì„±ëŠ¥ ìš°ì„  (ì—°êµ¬/ë²¤ì¹˜ë§ˆí‚¹)',
        'clinical_screening': 'ì„ìƒ ì„ ë³„ê²€ì‚¬ (ì‹¤ìš©ì„± ì¤‘ì‹œ)',
        'stability_priority': 'ì•ˆì •ì„± ìš°ì„  (ì‹ ë¢°ì„± ì¤‘ì‹œ)'
    }

    print("\n\n" + "=" * 80)
    print("ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœì  Image-bins ëª¨ë¸ ì¶”ì²œ (ê°œì„ ëœ ê¸°ì¤€ ì ìš©)")
    print("=" * 80)

    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ìƒìœ„ 3ê°œ ëª¨ë¸ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    for scenario_key, scenario_desc in scenarios_desc.items():
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬
        scores = {exp: rec[scenario_key] for exp, rec in recommendations.items()}
        sorted_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        print(f"\nğŸ† ì‹œë‚˜ë¦¬ì˜¤: {scenario_desc}")
        print("-" * 60)

        for i, (model_name, score) in enumerate(sorted_models[:3]):
            model_row = df[df['Experiment'] == model_name].iloc[0]
            marker = "â­" if i == 0 else f"  {i+1}."
            print(f"{marker} {model_name}")
            print(f"     - ì¢…í•© ì ìˆ˜: {score:.4f}")
            print(f"     - MAE: {model_row['Mean_MAE']:.4f} (Rank: {int(model_row['Mean_MAE_Rank'])})")
            print(f"     - Std Dev: {model_row['Std_Dev']:.4f} (Rank: {int(model_row['Std_Dev_Rank'])})")
            print(f"     - Complexity: {model_row['Complexity']:.3f} (Rank: {int(model_row['Complexity_Rank'])})")

    # ğŸ”¥ Bayesian Optimization ëª©í‘œì— ë”°ë¥¸ ìµœì¢… ì¶”ì²œ ëª¨ë¸ (MAE ìµœì†Œ)
    mae_best_model = df.loc[df['Mean_MAE'].idxmin(), 'Experiment']

    print("\n\n" + "=" * 80)
    print("âœ… ìµœì¢… Bayesian Optimization ì¶”ì²œ Image-bins ëª¨ë¸")
    print("=" * 80)
    print(f"ğŸ‰ {mae_best_model}")
    print("   - ì´ ëª¨ë¸ì€ Bayesian optimizationì˜ ëª©í‘œì¸ 'MAE ìµœì†Œí™”'ë¥¼ ë‹¬ì„±í•œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì…ë‹ˆë‹¤.")
    print("   - 44ê°œ image-bins ëª¨ë¸ ì¤‘ ê°€ì¥ ë‚®ì€ MAEë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.")

    return mae_best_model

def create_hpo_table_with_new_scores(df, recommendations):
    """HPO ìŠ¤íƒ€ì¼ì˜ í…Œì´ë¸”ì„ ìƒˆë¡œìš´ ì ìˆ˜ì™€ í•¨ê»˜ ìƒì„±"""
    # recommendations ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    scores_df = pd.DataFrame.from_dict(recommendations, orient='index')
    scores_df.rename(columns={
        'performance_priority': 'Score (Performance)',
        'clinical_screening': 'Score (Clinical)', 
        'stability_priority': 'Score (Stability)'
    }, inplace=True)

    # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ê³¼ ì ìˆ˜ ë°ì´í„°í”„ë ˆì„ ë³‘í•©
    final_df = df.merge(scores_df, left_on='Experiment', right_index=True, how='left')
    
    # HPO í…Œì´ë¸” ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ (foldë³„ MAE ì¶”ê°€)
    hpo_style_results = []
    
    for _, row in final_df.iterrows():
        fold_maes = row['Fold_MAEs']
        
        result_row = {
            'Experiment': row['Experiment'],
            '1st Fold': round(fold_maes[0], 4) if len(fold_maes) > 0 else None,
            '2nd Fold': round(fold_maes[1], 4) if len(fold_maes) > 1 else None,
            '3rd Fold': round(fold_maes[2], 4) if len(fold_maes) > 2 else None,
            '4th Fold': round(fold_maes[3], 4) if len(fold_maes) > 3 else None,
            '5th Fold': round(fold_maes[4], 4) if len(fold_maes) > 4 else None,
            'Mean_MAE': round(row['Mean_MAE'], 4),
            'Std_Dev': round(row['Std_Dev'], 4),
            '95%_CI_Range': row['95%_CI_Range'],
            'Model': 'image-bins',
            'Bins': row['Bins'],
            'CV': round(row['CV'], 4),
            'CI_Width': round(row['CI_Width'], 4),
            'Complexity': round(row['Complexity'], 4),
            'Score (Performance)': round(row['Score (Performance)'], 4),
            'Score (Clinical)': round(row['Score (Clinical)'], 4),
            'Score (Stability)': round(row['Score (Stability)'], 4),
            'Mean_MAE_Rank': int(row['Mean_MAE_Rank']),
            'Std_Dev_Rank': int(row['Std_Dev_Rank']),
            'Complexity_Rank': int(row['Complexity_Rank'])
        }
        hpo_style_results.append(result_row)
    
    hpo_df = pd.DataFrame(hpo_style_results)
    # MAE ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    hpo_df = hpo_df.sort_values('Mean_MAE')
    
    return hpo_df

def create_scatter_plot_grayscale(df, output_dir):
    """Image-bins HPO ê²°ê³¼ì˜ grayscale scatter plot ìƒì„±"""
    plt.figure(figsize=(15, 10))
    
    # Bins ê°’ê³¼ MAEë¡œ scatter plot ìƒì„±
    bins_values = df['Bins']
    mae_values = df['Mean_MAE']
    std_values = df['Std_Dev']
    
    # CI ê°’ ì¶”ì¶œ
    ci_lower = df['95%_CI_Range'].str.extract(r'\[([0-9.]+),').astype(float).values.flatten()
    ci_upper = df['95%_CI_Range'].str.extract(r', ([0-9.]+)\]').astype(float).values.flatten()
    
    # Grayscale scatter plot with error bars
    plt.errorbar(bins_values, mae_values, yerr=std_values,
                fmt='o', capsize=5, label='Image-bins (Std Dev)',
                color='black', alpha=0.7)
    
    # Add CI as filled area
    plt.fill_between(bins_values, ci_lower, ci_upper,
                   alpha=0.2, color='gray',
                   label='Image-bins (95% CI)')
    
    plt.xlabel('Bins', fontsize=12)
    plt.ylabel('Mean MAE', fontsize=12)
    plt.title('Image-bins Hyperparameter Optimization: Bins vs Mean MAE', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Save grayscale plot
    output_path = output_dir / 'hyperparameter_optimization_scatter_grayscale_imageBinsOnly.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'   ğŸ“Š Grayscale plot: {output_path}')

def save_enhanced_results(df, best_models, recommendations, best_overall_model):
    """í–¥ìƒëœ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì €ì¥"""
    output_dir = Path("report_250924")
    output_dir.mkdir(exist_ok=True)
    
    # 1. HPO ìŠ¤íƒ€ì¼ í…Œì´ë¸” with new scores ìƒì„± ë° ì €ì¥
    hpo_table = create_hpo_table_with_new_scores(df, recommendations)
    excel_path = output_dir / 'spp_table1_full_hyperparameter_search_mae_imageBinsOnly.xlsx'
    hpo_table.to_excel(excel_path, index=False)
    
    # 2. Grayscale scatter plot ìƒì„±
    create_scatter_plot_grayscale(df, output_dir)
    
    # 3. ê¸°ì¡´ ìš”ì•½ ê²°ê³¼ë“¤ë„ ì €ì¥
    df_output = df.copy()
    df_output = df_output.sort_values('Mean_MAE')
    df_output.to_excel(output_dir / 'image_bins_optimization_results_summary.xlsx', index=False)
    
    # Best models ìš”ì•½ ì €ì¥
    best_summary = []
    for key, model in best_models.items():
        best_summary.append({
            'Criteria': key.replace('_', ' ').title(),
            'Model': model['Experiment'],
            'MAE': model['Mean_MAE'],
            'Std_Dev': model['Std_Dev'],
            'Bins': model['Bins'],
            'Composite_Score': model['Composite_Score']
        })
    
    best_df = pd.DataFrame(best_summary)
    best_df.to_excel(output_dir / 'image_bins_best_models_summary.xlsx', index=False)
    
    print(f'\nğŸ’¾ í–¥ìƒëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:')
    print(f'   ğŸ“Š HPO ìŠ¤íƒ€ì¼ í…Œì´ë¸”: {excel_path}')
    print(f'   ğŸ“Š ì „ì²´ ê²°ê³¼: {output_dir}/image_bins_optimization_results_summary.xlsx')
    print(f'   ğŸ† Best models: {output_dir}/image_bins_best_models_summary.xlsx')
    print(f'   ğŸ¯ ìµœì¢… ì¶”ì²œ ëª¨ë¸: {best_overall_model}')

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. ëª¨ë“  image-bins ëª¨ë¸ ë¡œë“œ ë° ë¶„ì„
    df = load_and_analyze_image_bins_models()
    if df is None:
        return
    
    # 2. ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ë¶„ì„
    best_models = analyze_best_models(df)
    
    # 3. ìƒìœ„ ëª¨ë¸ë“¤ ìš”ì•½
    show_top_models_summary(df)
    
    # 4. Bins vs Performance ê´€ê³„ ë¶„ì„
    analyze_bins_vs_performance_trend(df)
    
    # 5. ìµœì¢… ì¶”ì²œ
    final_recommendation(df, best_models)
    
    # 6. ğŸ”¥ ê°œì„ ëœ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ í‰ê°€
    recommendations, enhanced_df = improved_model_selection_for_image_bins(df)
    
    # 7. ğŸ”¥ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    best_overall_model = display_new_criteria_results(enhanced_df, recommendations)
    
    # 8. ğŸ”¥ í–¥ìƒëœ ê²°ê³¼ ì €ì¥ (HPO í…Œì´ë¸” + Grayscale plot)
    save_enhanced_results(enhanced_df, best_models, recommendations, best_overall_model)
    
    print(f"\nğŸ‰ Image-bins ëª¨ë¸ ì„ íƒ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70)

if __name__ == "__main__":
    main()