#!/usr/bin/env python3
"""
Image-bins Model Selection from Hyperparameter Optimization Results
================================================================

This script analyzes image-bins hyperparameter optimization results to select the optimal model.
Finds the model with the best performance among various bins values generated by Bayesian optimization.

Created: 2025-09-22
"""

import glob
import os
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats


def load_and_analyze_image_bins_models():
    """Analyze all image-bins models in combineResults_from_ImageBinsHPO folder"""
    print("ğŸš€ Image-bins Model Selection from Bayesian Optimization Results")
    print("=" * 70)

    # Set working directory
    os.chdir("/home/erdrajh/project_a6000/project25/2506Anemia/kimsangwon_code_3")

    # Find all files in combineResults_from_ImageBinsHPO folder
    results_pattern = "combineResults_from_ImageBinsHPO/results_image-bins*_combined.xlsx"
    files = sorted(glob.glob(results_pattern))

    if not files:
        print(f"âŒ Files not found: {results_pattern}")
        return None

    print(f"ğŸ“ Found files: {len(files)}")
    for i, file in enumerate(files, 1):
        bins_num = extract_bins_from_filename(file)
        print(f"   {i:2d}. image-bins{bins_num} ({os.path.basename(file)})")

    # Load and analyze all files
    all_results = []

    for file in files:
        print(f"\nğŸ“– Analyzing: {os.path.basename(file)}")
        result = analyze_single_file(file)
        if result is not None:
            all_results.append(result)

    if not all_results:
        print("âŒ No files available for analysis.")
        return None

    # Convert results to DataFrame
    df = pd.DataFrame(all_results)

    return df


def extract_bins_from_filename(filename):
    """Extract bins value from filename"""
    import re

    match = re.search(r"image-bins(\d+)", filename)
    return int(match.group(1)) if match else 0


def analyze_single_file(file_path):
    """Analyze single file"""
    try:
        df = pd.read_excel(file_path)

        if "ground truth" not in df.columns or "prediction" not in df.columns:
            print(f"   âŒ Required columns missing: {file_path}")
            return None

        # Extract Bins value
        bins = extract_bins_from_filename(file_path)

        # Calculate MAE per fold
        fold_maes = {}
        fold_counts = {}

        for fold in range(5):  # 0-4 folds
            fold_data = df[df["fold"] == fold]
            if len(fold_data) > 0:
                mae = np.mean(np.abs(fold_data["ground truth"] - fold_data["prediction"]))
                fold_maes[fold] = mae
                fold_counts[fold] = len(fold_data)

        if not fold_maes:
            print(f"   âŒ No fold data: {file_path}")
            return None

        # Calculate statistics
        fold_mae_values = list(fold_maes.values())
        mean_mae = np.mean(fold_mae_values)
        std_mae = np.std(fold_mae_values, ddof=1)

        # Calculate RMSE
        rmse = np.sqrt(np.mean((df["ground truth"] - df["prediction"]) ** 2))

        # Calculate CV (coefficient of variation)
        cv = std_mae / mean_mae if mean_mae > 0 else 0

        # Calculate 95% CI
        if len(fold_mae_values) > 1:
            ci_range = stats.t.interval(0.95, len(fold_mae_values) - 1, loc=mean_mae, scale=stats.sem(fold_mae_values))
            ci_width = ci_range[1] - ci_range[0]
            ci_range_str = f"[{ci_range[0]:.4f}, {ci_range[1]:.4f}]"
        else:
            ci_width = 0
            ci_range_str = "[0.0000, 0.0000]"

        # Calculate Composite Score (performance + stability + simplicity)
        composite_score = mean_mae + 0.5 * std_mae + 0.1 * (bins / 100)

        result = {
            "Experiment": f"image-bins{bins}",
            "Bins": bins,
            "Mean_MAE": mean_mae,
            "Std_Dev": std_mae,
            "RMSE": rmse,
            "CV": cv,
            "95%_CI_Range": ci_range_str,
            "CI_Width": ci_width,
            "Composite_Score": composite_score,
            "Total_Samples": len(df),
            "Fold_Counts": fold_counts,
            "Fold_MAEs": fold_maes,
            "Source_File": file_path,
        }

        print(f"   âœ… bins{bins}: MAE {mean_mae:.4f} Â± {std_mae:.4f}")

        return result

    except Exception as e:
        print(f"   âŒ Analysis failed: {e}")
        return None


def analyze_best_models(df):
    """Analyze optimal models by various criteria"""
    print("\nğŸ“Š Best Model Analysis by Various Evaluation Metrics")
    print("=" * 60)

    # 1. MAE criterion (Bayesian Optimization target)
    print("\n1. ğŸ¯ MAE Criterion (Bayesian Optimization Target):")
    mae_best = df.loc[df["Mean_MAE"].idxmin()]
    print(f'   ğŸ† {mae_best["Experiment"]}')
    print(f'   ğŸ“ˆ MAE: {mae_best["Mean_MAE"]:.4f} Â± {mae_best["Std_Dev"]:.4f}')
    print(f'   ğŸ”¢ Bins: {mae_best["Bins"]}')

    # 2. RMSE criterion
    print("\n2. ğŸ“ RMSE Criterion (larger penalty for larger errors):")
    rmse_best = df.loc[df["RMSE"].idxmin()]
    print(f'   ğŸ† {rmse_best["Experiment"]}')
    print(f'   ğŸ“ˆ RMSE: {rmse_best["RMSE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {rmse_best["Bins"]}')

    # 3. Stability criterion
    print("\n3. ğŸ­ Stability Criterion (model with smallest standard deviation):")
    stability_best = df.loc[df["Std_Dev"].idxmin()]
    print(f'   ğŸ† {stability_best["Experiment"]}')
    print(f'   ğŸ“Š Std: {stability_best["Std_Dev"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {stability_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {stability_best["Bins"]}')

    # 4. CV criterion
    print("\n4. ğŸ“Š CV (Coefficient of Variation) Criterion:")
    cv_best = df.loc[df["CV"].idxmin()]
    print(f'   ğŸ† {cv_best["Experiment"]}')
    print(f'   ğŸ“ˆ CV: {cv_best["CV"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {cv_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {cv_best["Bins"]}')

    # 5. Composite Score
    print("\n5. ğŸ… Composite Score (performance + stability + simplicity):")
    composite_best = df.loc[df["Composite_Score"].idxmin()]
    print(f'   ğŸ† {composite_best["Experiment"]}')
    print(f'   ğŸ“Š Score: {composite_best["Composite_Score"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {composite_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {composite_best["Bins"]}')

    # 6. CI Width criterion
    print("\n6. ğŸ¯ Model with narrowest 95% CI range (most reliable):")
    ci_best = df.loc[df["CI_Width"].idxmin()]
    print(f'   ğŸ† {ci_best["Experiment"]}')
    print(f'   ğŸ“Š CI Width: {ci_best["CI_Width"]:.4f}')
    print(f'   ğŸ“ˆ MAE: {ci_best["Mean_MAE"]:.4f}')
    print(f'   ğŸ”¢ Bins: {ci_best["Bins"]}')

    # 7. Efficiency criterion (good performance + minimum bins)
    print("\n7. âš¡ Efficiency Criterion (good performance + minimum bins):")
    # Model with smallest bins while MAE is in top 25%
    mae_threshold = df["Mean_MAE"].quantile(0.25)
    efficient_models = df[df["Mean_MAE"] <= mae_threshold]
    if not efficient_models.empty:
        efficient_best = efficient_models.loc[efficient_models["Bins"].idxmin()]
        print(f'   ğŸ† {efficient_best["Experiment"]}')
        print(f'   ğŸ“ˆ MAE: {efficient_best["Mean_MAE"]:.4f} (top 25%)')
        print(f'   ğŸ”¢ Bins: {efficient_best["Bins"]} (minimum)')
    else:
        efficient_best = df.loc[df["Bins"].idxmin()]
        print(f'   ğŸ† {efficient_best["Experiment"]} (minimum bins)')
        print(f'   ğŸ“ˆ MAE: {efficient_best["Mean_MAE"]:.4f}')
        print(f'   ğŸ”¢ Bins: {efficient_best["Bins"]}')

    return {
        "mae_best": mae_best,
        "rmse_best": rmse_best,
        "stability_best": stability_best,
        "cv_best": cv_best,
        "composite_best": composite_best,
        "ci_best": ci_best,
        "efficient_best": efficient_best,
    }


def show_top_models_summary(df, top_n=5):
    """Display summary of top models"""
    print(f"\nğŸ“‹ Top {top_n} Models Summary")
    print("=" * 80)

    # Sort by MAE
    top_models = df.nsmallest(top_n, "Mean_MAE")

    print(f"{'Rank':<4} {'Model':<12} {'MAE':<8} {'Std':<8} {'RMSE':<8} {'CV':<8} {'Bins':<6} {'Composite':<10}")
    print("-" * 80)

    for i, (_, model) in enumerate(top_models.iterrows(), 1):
        print(
            f"{i:<4} {model['Experiment']:<12} {model['Mean_MAE']:<8.4f} "
            f"{model['Std_Dev']:<8.4f} {model['RMSE']:<8.4f} {model['CV']:<8.4f} "
            f"{model['Bins']:<6} {model['Composite_Score']:<10.4f}"
        )


def analyze_bins_vs_performance_trend(df):
    """Analyze relationship between Bins value and performance"""
    print(f"\nğŸ“ˆ Bins vs Performance Relationship Analysis")
    print("=" * 60)

    # Sort by Bins
    df_sorted = df.sort_values("Bins")

    # Correlation analysis
    bins_mae_corr = df["Bins"].corr(df["Mean_MAE"])
    bins_std_corr = df["Bins"].corr(df["Std_Dev"])

    print(f"ğŸ“Š Correlation Analysis:")
    print(f"   â€¢ Bins vs MAE correlation: {bins_mae_corr:.3f}")
    print(f"   â€¢ Bins vs Std correlation: {bins_std_corr:.3f}")

    print(f"\nğŸ“Š Performance by Bins Range:")

    # Analyze by dividing Bins into ranges
    bins_ranges = [
        (0, 20, "Very Low (0-20)"),
        (21, 40, "Low (21-40)"),
        (41, 60, "Medium (41-60)"),
        (61, 80, "High (61-80)"),
        (81, 100, "Very High (81-100)"),
    ]

    for min_bins, max_bins, label in bins_ranges:
        range_data = df[(df["Bins"] >= min_bins) & (df["Bins"] <= max_bins)]
        if not range_data.empty:
            avg_mae = range_data["Mean_MAE"].mean()
            avg_std = range_data["Std_Dev"].mean()
            best_in_range = range_data.loc[range_data["Mean_MAE"].idxmin()]
            print(
                f'   â€¢ {label}: Avg MAE {avg_mae:.4f}, Best: {best_in_range["Experiment"]} ({best_in_range["Mean_MAE"]:.4f})'
            )


def final_recommendation(df, best_models):
    """Final recommendation"""
    print(f"\nğŸ¯ Final Recommendation and Conclusion")
    print("=" * 60)

    # Best model by MAE criterion (Bayesian Optimization target)
    mae_best = best_models["mae_best"]

    print(f"ğŸ† Bayesian Optimization Target Achievement:")
    print(f'   âœ… Best performance model: {mae_best["Experiment"]}')
    print(f'   ğŸ“ˆ MAE: {mae_best["Mean_MAE"]:.4f} Â± {mae_best["Std_Dev"]:.4f}')
    print(f'   ğŸ”¢ Bins: {mae_best["Bins"]}')
    print(f"   ğŸ“Š Best performance among {len(df)} models")

    # Performance vs efficiency trade-off analysis
    composite_best = best_models["composite_best"]

    print(f"\nğŸ­ Balanced Recommendation:")
    if mae_best["Experiment"] == composite_best["Experiment"]:
        print(f'   âœ… Model with both best performance and balance: {mae_best["Experiment"]}')
        print(f"   ğŸ’¡ Achieves optimal performance and efficiency simultaneously with single model")
    else:
        print(f'   âš–ï¸ Best balanced model: {composite_best["Experiment"]}')
        print(f'   ğŸ“ˆ MAE: {composite_best["Mean_MAE"]:.4f} (vs best {mae_best["Mean_MAE"]:.4f})')
        print(f'   ğŸ”¢ Bins: {composite_best["Bins"]} (vs best {mae_best["Bins"]})')

        mae_diff = composite_best["Mean_MAE"] - mae_best["Mean_MAE"]
        bins_diff = mae_best["Bins"] - composite_best["Bins"]

        print(f"   ğŸ“Š Trade-off: Save {bins_diff} bins at cost of {mae_diff:+.4f} g/dL MAE")

    # Final recommendation
    print(f"\nğŸ’¡ Recommendations by Use Case:")
    print(f'   ğŸ¯ When best performance needed: {mae_best["Experiment"]} (MAE {mae_best["Mean_MAE"]:.4f})')
    print(
        f'   âš¡ When efficiency considered: {best_models["efficient_best"]["Experiment"]} (Bins {best_models["efficient_best"]["Bins"]})'
    )
    print(
        f'   ğŸ­ When stability prioritized: {best_models["stability_best"]["Experiment"]} (Std {best_models["stability_best"]["Std_Dev"]:.4f})'
    )
    print(f'   ğŸ… Overall balance: {composite_best["Experiment"]} (Score {composite_best["Composite_Score"]:.4f})')


def improved_model_selection_for_image_bins(df):
    """
    Evaluate and score image-bins models by applying improved criteria (complexity, scenarios).
    """
    # 1. Calculate additional basic metrics
    if "CV" not in df.columns:
        df["CV"] = df["Std_Dev"] / df["Mean_MAE"]

    # Calculate CI_Width (extracted from 95% CI range)
    df["CI_Width"] = (
        df["95%_CI_Range"]
        .str.extract(r"\[([0-9.]+),\s*([0-9.]+)\]")
        .astype(float)
        .apply(lambda x: x[1] - x[0], axis=1)
    )

    # 2. ğŸ”¥ Calculate Image-bins specific Complexity
    # image-bins model is single modality, so complexity mainly depends on number of bins
    df["Complexity"] = df["Bins"] / df["Bins"].max()

    # 3. Calculate ranks (lower is better)
    rank_columns = ["Mean_MAE", "Std_Dev", "CV", "CI_Width", "Complexity"]
    for col in rank_columns:
        df[f"{col}_Rank"] = df[col].rank(method="min")

    num_models = len(df)
    recommendations = {exp: {} for exp in df["Experiment"]}

    # 4. ğŸ”¥ Image-bins specific scenario-based weights
    scenarios = {
        "performance_priority": {  # Research/benchmarking: performance is most important
            "Mean_MAE": 0.8,
            "Std_Dev": 0.1,
            "Complexity": 0.1,
        },
        "clinical_screening": {  # ğŸ”¥ Clinical screening: balance between practicality (simplicity) and performance is important
            "Mean_MAE": 0.4,
            "CV": 0.2,
            "Complexity": 0.4,
        },
        "stability_priority": {  # Stability priority: consistency and reliability of predictions are most important
            "Mean_MAE": 0.3,
            "CV": 0.3,
            "CI_Width": 0.4,
        },
    }

    # 5. Calculate scores
    for scenario_name, weights in scenarios.items():
        for _, model in df.iterrows():
            score = 0
            for metric, weight in weights.items():
                rank_col = f"{metric}_Rank"
                # Convert rank to score between 0-1 (rank 1 gets highest score)
                normalized_score = (num_models - model[rank_col] + 1) / num_models
                score += normalized_score * weight

            recommendations[model["Experiment"]][scenario_name] = score

    return recommendations, df


def display_new_criteria_results(df, recommendations):
    """Organize and output improved criteria analysis results by scenario."""

    scenarios_desc = {
        "performance_priority": "Performance Priority (Research/Benchmarking)",
        "clinical_screening": "Clinical Screening (Practicality Focused)",
        "stability_priority": "Stability Priority (Reliability Focused)",
    }

    print("\n\n" + "=" * 80)
    print("ğŸ¯ Optimal Image-bins Model Recommendations by Scenario (Improved Criteria Applied)")
    print("=" * 80)

    # Output top 3 models for each scenario
    for scenario_key, scenario_desc in scenarios_desc.items():
        # Sort models by scenario-specific scores
        scores = {exp: rec[scenario_key] for exp, rec in recommendations.items()}
        sorted_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        print(f"\nğŸ† Scenario: {scenario_desc}")
        print("-" * 60)

        for i, (model_name, score) in enumerate(sorted_models[:3]):
            model_row = df[df["Experiment"] == model_name].iloc[0]
            marker = "â­" if i == 0 else f"  {i+1}."
            print(f"{marker} {model_name}")
            print(f"     - Overall score: {score:.4f}")
            print(f"     - MAE: {model_row['Mean_MAE']:.4f} (Rank: {int(model_row['Mean_MAE_Rank'])})")
            print(f"     - Std Dev: {model_row['Std_Dev']:.4f} (Rank: {int(model_row['Std_Dev_Rank'])})")
            print(f"     - Complexity: {model_row['Complexity']:.3f} (Rank: {int(model_row['Complexity_Rank'])})")

    # ğŸ”¥ Final recommended model according to Bayesian Optimization target (minimum MAE)
    mae_best_model = df.loc[df["Mean_MAE"].idxmin(), "Experiment"]

    print("\n\n" + "=" * 80)
    print("âœ… Final Bayesian Optimization Recommended Image-bins Model")
    print("=" * 80)
    print(f"ğŸ‰ {mae_best_model}")
    print(
        "   - This model is the best performing model that achieved 'MAE minimization', the goal of Bayesian optimization."
    )
    print("   - Recorded the lowest MAE among 44 image-bins models.")

    return mae_best_model


def create_hpo_table_with_new_scores(df, recommendations):
    """Create HPO-style table with new scores"""
    # Convert recommendations dictionary to DataFrame
    scores_df = pd.DataFrame.from_dict(recommendations, orient="index")
    scores_df.rename(
        columns={
            "performance_priority": "Score (Performance)",
            "clinical_screening": "Score (Clinical)",
            "stability_priority": "Score (Stability)",
        },
        inplace=True,
    )

    # Merge original DataFrame with scores DataFrame
    final_df = df.merge(scores_df, left_on="Experiment", right_index=True, how="left")

    # Convert to HPO table style (add fold-wise MAE)
    hpo_style_results = []

    for _, row in final_df.iterrows():
        fold_maes = row["Fold_MAEs"]

        result_row = {
            "Experiment": row["Experiment"],
            "1st Fold": round(fold_maes[0], 4) if len(fold_maes) > 0 else None,
            "2nd Fold": round(fold_maes[1], 4) if len(fold_maes) > 1 else None,
            "3rd Fold": round(fold_maes[2], 4) if len(fold_maes) > 2 else None,
            "4th Fold": round(fold_maes[3], 4) if len(fold_maes) > 3 else None,
            "5th Fold": round(fold_maes[4], 4) if len(fold_maes) > 4 else None,
            "Mean_MAE": round(row["Mean_MAE"], 4),
            "Std_Dev": round(row["Std_Dev"], 4),
            "95%_CI_Range": row["95%_CI_Range"],
            "Model": "image-bins",
            "Bins": row["Bins"],
            "CV": round(row["CV"], 4),
            "CI_Width": round(row["CI_Width"], 4),
            "Complexity": round(row["Complexity"], 4),
            "Score (Performance)": round(row["Score (Performance)"], 4),
            "Score (Clinical)": round(row["Score (Clinical)"], 4),
            "Score (Stability)": round(row["Score (Stability)"], 4),
            "Mean_MAE_Rank": int(row["Mean_MAE_Rank"]),
            "Std_Dev_Rank": int(row["Std_Dev_Rank"]),
            "Complexity_Rank": int(row["Complexity_Rank"]),
        }
        hpo_style_results.append(result_row)

    hpo_df = pd.DataFrame(hpo_style_results)
    # Sort by MAE
    hpo_df = hpo_df.sort_values("Mean_MAE")

    return hpo_df


def create_scatter_plot_grayscale(df, output_dir):
    """Create grayscale scatter plot of Image-bins HPO results"""
    plt.figure(figsize=(15, 10))

    # Create scatter plot with Bins values and MAE
    bins_values = df["Bins"]
    mae_values = df["Mean_MAE"]
    std_values = df["Std_Dev"]

    # Extract CI values
    ci_lower = df["95%_CI_Range"].str.extract(r"\[([0-9.]+),").astype(float).values.flatten()
    ci_upper = df["95%_CI_Range"].str.extract(r", ([0-9.]+)\]").astype(float).values.flatten()

    # Grayscale scatter plot with error bars
    plt.errorbar(
        bins_values,
        mae_values,
        yerr=std_values,
        fmt="o",
        capsize=5,
        label="Image-bins (Std Dev)",
        color="black",
        alpha=0.7,
    )

    # Add CI as filled area
    plt.fill_between(bins_values, ci_lower, ci_upper, alpha=0.2, color="gray", label="Image-bins (95% CI)")

    plt.xlabel("Bins", fontsize=12)
    plt.ylabel("Mean MAE", fontsize=12)
    plt.title("Image-bins Hyperparameter Optimization: Bins vs Mean MAE", fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Save grayscale plot
    output_path = output_dir / "hyperparameter_optimization_scatter_grayscale_imageBinsOnly.png"
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    print(f"   ğŸ“Š Grayscale plot: {output_path}")


def save_enhanced_results(df, best_models, recommendations, best_overall_model):
    """Save enhanced results comprehensively"""
    output_dir = Path("report_250924")
    output_dir.mkdir(exist_ok=True)

    # 1. Create and save HPO-style table with new scores
    hpo_table = create_hpo_table_with_new_scores(df, recommendations)
    excel_path = output_dir / "spp_table1_full_hyperparameter_search_mae_imageBinsOnly.xlsx"
    hpo_table.to_excel(excel_path, index=False)

    # 2. Create grayscale scatter plot
    create_scatter_plot_grayscale(df, output_dir)

    # 3. Also save existing summary results
    df_output = df.copy()
    df_output = df_output.sort_values("Mean_MAE")
    df_output.to_excel(output_dir / "image_bins_optimization_results_summary.xlsx", index=False)

    # Save best models summary
    best_summary = []
    for key, model in best_models.items():
        best_summary.append(
            {
                "Criteria": key.replace("_", " ").title(),
                "Model": model["Experiment"],
                "MAE": model["Mean_MAE"],
                "Std_Dev": model["Std_Dev"],
                "Bins": model["Bins"],
                "Composite_Score": model["Composite_Score"],
            }
        )

    best_df = pd.DataFrame(best_summary)
    best_df.to_excel(output_dir / "image_bins_best_models_summary.xlsx", index=False)

    print(f"\nğŸ’¾ Enhanced results saved:")
    print(f"   ğŸ“Š HPO-style table: {excel_path}")
    print(f"   ğŸ“Š Full results: {output_dir}/image_bins_optimization_results_summary.xlsx")
    print(f"   ğŸ† Best models: {output_dir}/image_bins_best_models_summary.xlsx")
    print(f"   ğŸ¯ Final recommended model: {best_overall_model}")


def main():
    """Main execution function"""

    # 1. Load and analyze all image-bins models
    df = load_and_analyze_image_bins_models()
    if df is None:
        return

    # 2. Analyze optimal models by various criteria
    best_models = analyze_best_models(df)

    # 3. Summarize top models
    show_top_models_summary(df)

    # 4. Analyze Bins vs Performance relationship
    analyze_bins_vs_performance_trend(df)

    # 5. Final recommendation
    final_recommendation(df, best_models)

    # 6. ğŸ”¥ Evaluate models with improved criteria
    recommendations, enhanced_df = improved_model_selection_for_image_bins(df)

    # 7. ğŸ”¥ Output scenario-based analysis results
    best_overall_model = display_new_criteria_results(enhanced_df, recommendations)

    # 8. ğŸ”¥ Save enhanced results (HPO table + Grayscale plot)
    save_enhanced_results(enhanced_df, best_models, recommendations, best_overall_model)

    print(f"\nğŸ‰ Image-bins model selection analysis completed!")
    print("=" * 70)


if __name__ == "__main__":
    main()
